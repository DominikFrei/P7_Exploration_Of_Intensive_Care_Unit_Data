---
title: "P7_Survival_After_Indwelling_Arterial_Catheters_Use"
author: "Dominik Frei"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=10, fig.height=5, fig.fullwidth=TRUE)
```

## Abstract

We run inference models for: 
- death vs censoring (logistic regression) / or death in hospital?
- for the group that died, day of death (linear regression)

## Resources

Data Source: https://physionet.org/content/mimic2-iaccd/1.0/


# Code

## Set up

Libraries:
```{r message=FALSE}
library(tidyverse)
library(knitr)
library(caret)
library(Amelia) #missmap()
library(mice) #imputation
library(Hmisc) #rcorr()
library(ggcorrplot) #ggcorrplot()
library(car) #vif()
library(caret)
library(olsrr)
```
Language:
```{r}
Sys.setenv(LANG = "en")
```

## Load and Preprocess Data

Load in the data set:
```{r}
data <- read.csv(file = "Data/full_cohort_data.csv",
                 stringsAsFactors = TRUE)
```

Convert all the binary variables into factors:
```{r define factors}
for (i in 1:ncol(data)) {
        if(length(unique(data[,i])) == 2) {
                data[,i] <- as.factor(data[,i])
        }
}
```


Display the data structure as imported:
```{r}
str(data)
```
`gender_num` and `sepsis_flg` should also be factors:
```{r}
data$gender_num <- as.factor(data$gender_num)
data$sepsis_flg <- as.factor(data$sepsis_flg)

data[,c("gender_num", "sepsis_flg")] %>% str()
```
`sepsis_flg` will not be useful in our analysis, since all the values are the same
(it has zero variance). We can therefore drop it from our data:
```{r}
data <- data %>% select(!sepsis_flg)
```

## Exploratory Data Analysis

### Missing Values

Lets look at the missing value sof the data:
```{r missmap all}
missmap(data[,colSums(is.na(data)) > 0], main = "Missingness Map: All Patients")
```
The `bmi` variable contains around 400 NA values, which have adjacent indexes.
This is somewhat odd. It does not seem that the data was ordered for missing values
in `bmi`, since there are other NAs, outside of the missing chunk.

Is there anything special about these missing records?

The variable `weight_first` might be somehow related to bmi. Lets plot it versus the rowindex:
```{r}
data %>% ggplot(aes(x = as.numeric(rownames(.)), y = weight_first)) +
        geom_point() +
        theme_classic()
```
There doesnt seem to be a particular order to the weights in the dataframe.

Lets also include `bmi` in the plot:
```{r}
data %>% ggplot(aes(x = as.numeric(rownames(.)))) +
        geom_point(aes(y = weight_first)) +
        geom_point(aes(y = bmi, col = "red")) +
        theme_classic()
```
Here we see that the missing chunk for the is at the end of the data frame.
It starts at the row number 1387.

Does this coincide with any patterns in other variables?

Lets calculate means for all the numeric variables, for the part of the data, where
the `bmi` values are missing and the rest and then compare them:
```{r}
mean_missing <- data %>% select_if(is.numeric) %>% 
        filter(as.numeric(rownames(.)) >= 1387) %>% 
        summarise(colname = colnames(.), mean = colMeans(., na.rm = TRUE)) %>% 
        mutate(in_chunk = TRUE)
        

mean_missing <- data %>% select_if(is.numeric) %>% 
        filter(as.numeric(rownames(.)) < 1387) %>% 
        summarise(colname = colnames(.), mean = colMeans(., na.rm = TRUE)) %>% 
        mutate(in_chunk = FALSE) %>% 
        rbind(mean_missing)
        
mean_missing %>% ggplot(aes(x = colname, y = mean, fill = as.factor(in_chunk))) +
        geom_bar(stat = "identity", position = "dodge") +
        theme_classic() +
        coord_flip()
```
There arent any big differences in the means between the part of the dataframe, where `bmi`
is missing and the rest.

Lets look at the factor variables:
```{r}
mean_missing_f <- data %>% select_if(is.factor) %>% 
        sapply(.,as.numeric) %>% 
        as.data.frame() %>% 
        filter(as.numeric(rownames(.)) >= 1387) %>% 
        summarise(colname = colnames(.), mean = colMeans(., na.rm = TRUE)) %>% 
        mutate(in_chunk = TRUE)

mean_missing_f <- data %>% select_if(is.factor) %>% 
        sapply(.,as.numeric) %>% 
        as.data.frame() %>%
        filter(as.numeric(rownames(.)) < 1387) %>% 
        summarise(colname = colnames(.), mean = colMeans(., na.rm = TRUE)) %>% 
        mutate(in_chunk = FALSE) %>% 
        rbind(mean_missing_f)

mean_missing_f %>% ggplot(aes(x = colname, y = mean, fill = as.factor(in_chunk))) +
        geom_bar(stat = "identity", position = "dodge") +
        theme_classic() +
        coord_flip()
```
Here we just converted the factor levels into numeric values and then took the means.
This is only a quick solution. For factors with more than two levels, this doesnt 
necessarily show the differences perfectly.

It  seems here as well there are no obvious patterns in the other variables.

Now we will produce a dummy variable that indicates the records, where the `bmi`
value is missing and then calculate the pearson correlation of this with all
other variables:
```{r}
#produce missingness indicator variable
data_mis_ind <- data %>% mutate(bmi_missing = as.numeric(is.na(data$bmi)))

#select all the numeric variables
data_mis_ind_n <- data_mis_ind %>% select_if(is.numeric)

#transform all the factors to numeric
data_mis_ind <- data_mis_ind %>% select_if(is.factor) %>% 
        sapply(.,as.numeric) %>% 
        as.data.frame() %>%
        cbind(., data_mis_ind_n)

#perform cor tests and save results
mis_cor_results <- data.frame(matrix(nrow = 0, ncol = 3))

for (i in 1:(ncol(data_mis_ind)-1)) {
        test <- cor.test(data_mis_ind$bmi_missing, data_mis_ind[,i], method = "pearson")
        res <- cbind(colnames(data_mis_ind)[i], test$estimate[[1]], test$p.value)
        mis_cor_results <- rbind(mis_cor_results, res)
}

#rename columns
colnames(mis_cor_results) <- c("var_name", "cor", "p_val")

#order for p value and present
mis_cor_results %>% arrange(p_val) %>% filter(p_val < 0.05) %>% kable()
```
There arent any strong correlations.

The reason why exactly these values are missing remains unclear.

Is there any correlation between `weight_first` and `bmi`?
```{r}
data %>% ggplot(aes(x = weight_first, y = bmi)) +
        geom_point() +
        theme_classic()
```

```{r}
cor.test(data$bmi, data$weight_first)
```
There is significant correlation between `weight_first` and `bmi` (which is not
surprising, since bmi is calculated using weight and height). The remaining variation
in the `bmi` is due to variances in height of the people. Unfortunately we dont know height.
Since the two variables have a strong correlation, it probably makes sense to just drop `bmi`.
Since we probably would have to choose between the two anyway if we want to fit a linear model.

Lets look at the rest of the missing values. In the missingness plot, above there does not 
seem to be too much of a pattern. There are some patients with several missing values.
The reason for this is probably that these variables are normally captured/recorded together
(e.g. blood co2 and oxygen levels or anamnesis => all the `_flg` variables) and where not
captured/recorded for certain patients.

Show counts of missing values:
```{r}
sapply(data, function(x) sum(is.na(x))) %>% 
        .[which(. > 0)] %>% 
        sort(decreasing = TRUE) %>% 
        data.frame()
```
For the variable with the next highest amount of missing values (po2_first), there
are around 10 % of the data missing. This is not ideal, but we will leave the variables
in.

We could have a look if the values where not captured due to early death of the
patients.
```{r}
data %>% filter(is.na(.$po2_first)) %>% select(icu_exp_flg) %>% table()
data %>% filter(is.na(.$iv_day_1)) %>% select(icu_exp_flg) %>% table()
data %>% filter(is.na(.$weight_first)) %>% select(icu_exp_flg) %>% table()
data %>% filter(is.na(.$sapsi_first)) %>% select(icu_exp_flg) %>% table()
```
This doesn't seem to be the case (at least not systematically), since not that many 
patients for which there are NA values died in the icu.

We will later impute these missing values.

### Variance

Lets look at the variablility in certain variables. Especially for the "flag variables"
I suspect there could be a low number of flagged patients, which can lead to problems (near zero variance).

Calculate the percentage in the data that have a positive (1) value for the flag
variables:
```{r}
data %>% select(ends_with("_flg")) %>% 
        sapply(function(x) as.numeric(x)-1) %>% 
        data.frame() %>% 
        colMeans() %>% 
        sort() %>% 
        data.frame() %>% 
        mutate(perc_flagged = .*100) %>% 
        select(!.) %>% 
        kable(digits = 2)
```
Some of the variables really do have a low variance e.g. `renal_flg` only contains 
ca. 3 percent of positive values.

We can use the caret::nearZeroVar function to identify near zero variables in
all of the data:
```{r}
nearZeroVar(data, saveMetrics = TRUE) %>% filter(zeroVar == TRUE | nzv == TRUE)
```
Here `renal_flg` is found to be the only near zero variable in the data set 
(we had already removed `sepsis_flg` earlier, which had zero variance).

### Variables with Information about Death

Some of the flag variables above are actually not flags for for medical conditions,
but contain information about death of patients or if an IAC was used on them:

- `censor_flg`: censored or death (binary: 0 = death, 1 = censored)  
- `day_28_flg`: death within 28 days (binary: 1 = yes, 0 = no)  
- `hosp_exp_flg`: death in hospital (binary: 1 = yes, 0 = no)
- `icu_exp_flg`: death in ICU (binary: 1 = yes, 0 = no)  
- `aline_flg`: IAC used (binary, 1 = yes, 0 = no) 

Lets plot a correlation matrix:
```{r}
#subset for these variables
data_death <- data %>% select(censor_flg, day_28_flg, hosp_exp_flg, icu_exp_flg, mort_day_censored, aline_flg)
#correlation matrix
rcorr(as.matrix(data_death))
```

`aline_flg` correlates significantly with `hosp_exp_flg` and `icu_exp_flg` and close
to significantly with `day_28_flg`. The correlation coefficients however are quite small
for all of them. This indicates that either the use of an AIC increases the risk for
death or patients with a higher risk of death are more likely to receive monitoring via IAC.

Lets look at contingency tables between `day_28_flg` and `hosp_exp_flg` / `icu_exp_flg`?
Can `hosp_exp_flg` / `icu_exp_flg` only be positive if `day_28_flg` is positive?

Produce contingency tables:
```{r}
table(data$day_28_flg, data$hosp_exp_flg, dnn = c("day_28_flg", "hosp_exp_flg"))

table(data$day_28_flg, data$icu_exp_flg, dnn = c("day_28_flg", "icu_exp_flg"))
```
No, there are three patients that died in the hospital, but not within 28 days.

However from the correlations of `day_28_flg` with `hosp_exp_flg` / `icu_exp_flg`
we see that many of them died in the ICU or in the hospital

Lets look at the contingency table of `day_28_flg` with `censor_flg`:
```{r}
table(data$day_28_flg, data$censor_flg, dnn = c("day_28_flg", "censor_flg"))
```
We see that there are no discrepancies between the variables (dead within 28 days but not
registered as dead in `censor_flg`)

Other then the flag variables discussed above there is also some information about
how many days after ICU admission the person died or was censored (`mort_day_censored`).
Create plot of the distributions over time for dead and censored patients:
```{r fig.height=5}
data %>% ggplot(aes(x = mort_day_censored, fill = censor_flg, alpha = 0.5))+
        geom_density()+
        theme_classic()+
        geom_vline(xintercept = 120, col = "green") +
        scale_x_continuous(breaks=c(seq(from = 0, to = 3000, by=250)))
```
We can see on the plot above, that deaths where registered up until ca. 750 days
after ICU admission, with most of them happening within maybe up until 120 days
(here marked by the green vertical line)
and that starting at around 500 days after ICU admission censoring starts.

Lets plot the distribution above for only patients that had IAC:
```{r fig.height=5}
data %>% filter(aline_flg == 1) %>% 
        ggplot(aes(x = mort_day_censored, fill = censor_flg, alpha = 0.5))+
        geom_density()+
        geom_vline(xintercept = 28, col = "red") +
        geom_vline(xintercept = 120, col = "green") +
        theme_classic()+
        scale_x_continuous(breaks=c(seq(from = 0, to = 3000, by=250)))
```
We can see that most deaths happened up until maybe 120 days after ICU admission
(here marked with the green line). There is another small peak between 500 and 700
days. Censoring also starts at around 500 days. Over all the distributions are quite similar
to the ones in the plot above.  
The red vertical line in the plot marks the cut of for the inclusion in the `day_28_flg`
variable. This was used as main outcome in the analysis performed in the book.
To my understanding this cut of is more or less arbitrary and was set / or chosen
as to be in close proximity to ICU admission. I assume this was done, to due to the assumption
that if use of an IAC would increase mortality it would most likely be short term mortality
(due to more infections or similar).  
Since we have the necessary data we could create flags with other cut offs.

### Physiological Measurements

Lets look at all the variables that contain physiological measurements or similar 
(ending in _1st or _first):
```{r}
#subset
data_meas <- data %>% select(ends_with("_1st") | ends_with("_first"))

#scale
data_meas <- data_meas %>% sapply(., scale) %>% data.frame()

#pivot longer and create indicators for outliers
data_meas <- data_meas %>% pivot_longer(cols = colnames(.)) %>% 
        group_by(name) %>% 
        mutate(outlier = as.factor(ifelse((value>3*IQR(value, na.rm = TRUE) | value <(-3*IQR(value, na.rm = TRUE))), 1, 0)))
```

Produce density plots to present the distribution of the data (outliers (>3*IQR) are marked
by red ticks)
```{r fig.height=5}
data_meas %>% filter(name %in% unique(.$name)[1:6]) %>% 
        ggplot(aes(x = value)) +
        facet_wrap(~ name) +
        geom_density() +
        geom_rug(data = ~ filter(.x, outlier == 1), col = "red") +
        coord_cartesian(ylim = c(0, 1)) +
        theme_bw()
```
- `spo2_1st` is left skewed. The variable measures Saturation of Peripheral Oxygen
in percent, which normally is between 95% â€“ 99%. So the distribution here makes sense. 

Lets look at the lowest values:
```{r}
data %>% arrange(spo2_1st) %>% select(spo2_1st) %>% .[1:20,]
```
There are some very low values. Lets look at the records with values below 50:
```{r}
data %>% filter(spo2_1st < 50) %>%  select(spo2_1st, po2_first, icu_los_day, 
                                           hospital_los_day, sapsi_first, sofa_first, 
                                           service_num, hosp_exp_flg, icu_exp_flg, 
                                           day_28_flg, mort_day_censored, copd_flg,
                                           resp_flg) %>% kable()
```
We can see that 4 of the 6 patients survived.

Three of the patients had respiratory diseases (non COPD).

The po2 scores are not especially low, only one value is below 80 mmHg.
I found some literature (https://journals.lww.com/ccmjournal/Abstract/2012/12001/414__CORRELATION_OF_PULSE_OXIMETRY_AND_ARTERIAL.379.aspx) that states that the two values where found to be correlated in 
most cases (94%) for values of pO2 >60 mmHg.  
However it is also quite possible that
the spo2 levels where measured first (as it can be measured non invasively and quickly) 
and the po2 values where measured only after 
some measures where taken (ventilation with oxygen). This could also explain
the relatively high po2 levels in most patients.

The SAPS I scores are not that high (all < 29). For the SAPS II score, this would
indicate a mortality rate of < 10 %
according to (https://clincalc.com/IcuMortality/SAPSII.aspx).
I did not find anything about SAPS I in a quick search.
It has to be noted, that SPO2 is not used in the calculation of SAPS II.

For the SOFA score the maximal value is 24, so the values are not that critical either.
SPO2 is not used in the calculation of the SOFA score either.

We will assume, that these measurements are wrong and set them to NA to impute them later:
```{r}
data[which(data$spo2_1st < 50), "spo2_1st"] <- NA
```

- There are some high values for `weight_first`, which is plausible (obese patients).  
- There is an extremely low value for `temp_1st`. lets have a look at it:
```{r}
data$temp_1st %>% min(., na.rm = TRUE)
```
Temperature is coded in Fahrenheit. In Celsius it is:
```{r}
(32-32)/1.8
```
This is clearly impossible, therefore we will replace this value with NA and will impute it later
together with the other data:
```{r}
data[data$temp_1st == 32 & is.na(data$temp_1st) == FALSE, "temp_1st"] <- NA
```

The other distributions are more or less symmetric.
```{r fig.height=5}
data_meas %>% filter(name %in% unique(.$name)[7:12]) %>% 
        ggplot(aes(x = value)) +
        facet_wrap(~ name) +
        geom_density() +
        geom_rug(data = ~ filter(.x, outlier == 1), col = "red") +
        theme_bw()
```
All are more or less symmetric.
For these lab values, especially high or low values can be indicative of underlying
diseases.

Lets look at the outliers of `wbc_first` (first White blood cell count (K/uL, numeric)),
as they are the most extreme:
```{r}
data$wbc_first %>% sort(decreasing = TRUE) %>% head(n = 3)
```
Norm values are from 4.5 - 11.0 K/uL. So these values are clearly quite out of the normal range.
This can be indicative of infections or malignancies.

We will not look more closely at the other values.

```{r fig.height=5}
data_meas %>% filter(name %in% unique(.$name)[13:18]) %>% 
        ggplot(aes(x = value)) +
        facet_wrap(~ name) +
        geom_density() +
        geom_rug(data = ~ filter(.x, outlier == 1), col = "red") +
        theme_bw()
```
The distribution for `po2_first` is not very symetric.

- `po2_first`: Partial Pressure of Oxygen in blood (mmHg, numeric): high values dont
have a specific medical implication. They could also be explained by ventilation 
with oxygen before the measurement.  

There are lots of outliers for `pco2_first`, `bun_first` and `creatinine_first`.

- `pco2_first`: first PaCO_2 (mmHg, numeric). High values can be the result of hypoventilation
which could make sense in the context of ICU admission.

- `bun_first`: first Blood urea nitrogen (mg/dL, numeric): higher values could indicate
 decrease in glomerular filtration rate, which in turn is suggestive of kidney failure.
- The same goes for `creatinine_first`: first Creatinine (mg/dL, numeric)

We would expect a certain correlation of `bun_first` with `renal_flg`. Lets check this:
```{r}
cor.test(data$bun_first, as.numeric(data$renal_flg))
```
There is a statistically significant correlation between the two.

### Other Variables

There is some information about how long the patients stayed in ICU / hospital:

- `icu_los_day`: length of stay in ICU (days, numeric)  
- `hospital_los_day`: length of stay in hospital (days, numeric)  

Lets plot length of stay in ICU for dead and censored patients:
```{r fig.height=5}
data %>% ggplot(aes(x = icu_los_day, fill = censor_flg, alpha = 0.5))+
        geom_density()+
        theme_classic()
```
The patients that died had generally a bit longer ICU stays and most patients 
that stayed longer then 20 days died.

Lets plot length of stay in the ICU for patients with and without IAC:
```{r fig.height=5}
data %>% ggplot(aes(x = icu_los_day, fill = aline_flg, alpha = 0.5))+
        geom_density()+
        theme_classic()
```
Patients that stayed longer where more likely to receive an IAC. This makes sense, 
as it is a convenient way to administer liquids intravenously.  
This indicates that there might be some confounding between ICU stay and IAC with
death, since patients with longer stays where more likely to die and more likely to
receive an IAC.

Lets see if these tendencies are the same for length of hospital stay:
```{r fig.height=5}
data %>% ggplot(aes(x = hospital_los_day, fill = paste(aline_flg, censor_flg), alpha = 0.2))+
        geom_density()+
        theme_classic()
```
Here it looks like patients that died had shorter stays (since their stay might have
been terminated by their death), at least the patients that received an IAC.

Here also patients with IAC had generally longer stays.

Since the two variables measure similar things we will take a look at how correlated
they are:
```{r}
cor.test(data$icu_los_day, data$hospital_los_day)
```
There is a significant correlation between the two.

It might make sense to calculate the difference between the two to get the length of
the hospital stay after they left the ICU:
```{r}
data <- data %>% mutate(hospital_los_after_icu = hospital_los_day - icu_los_day)
```

Lets look at the correlation between the new variable and the length of stay in the icu:
```{r}
cor.test(data$icu_los_day, data$hospital_los_after_icu)
```
There is still significant correlation, but its magnitude is lower.
It might make sense to replace the `hospital_los_day` with this new variable.

Lets look how those two correlate:
```{r}
cor.test(data$hospital_los_day, data$hospital_los_after_icu)
```
The correlation is quite strong, so they probably should not both be included.

Lets look at the distribution of the new variable:
```{r}
summary(data$hospital_los_after_icu)
```
There is at least one negative value. Lets look at some of these records:
```{r}
data %>% filter(hospital_los_after_icu < 0) %>% 
        select(hospital_los_after_icu,
        hospital_los_day, icu_los_day, hour_icu_intime, 
        mort_day_censored, icu_exp_flg) %>% 
        arrange(hospital_los_after_icu) %>% 
        head() %>% 
        kable()
```
It looks like the stay in the ICU might have been recorded in hours and then 
transformed to days, since the values are not given as integers. The same goes
for the time until death / censoring. Those where probably calculated from the 
`hour_icu_intime`, which give the time of admission in ICU.

In most cases this leads to a negative value between -1 and 0. Those could just be set to zero.
However there are three values where there must be a mistake in one of the two
"length of stay" variables, since we get lower negative values. Unfortunately, We cannot draw 
any conclusion about which of the values is wrong from `mort_day_censored` either,
since they dont coincide with any of the two "length of stay" variables (the patients
seem to have survived).

It is also worth to be noted, that the stay in ICU for the three patients that died in the ICU
was recorded as almost one day longer then the recorded time until death. This could
be explained if we assume that the recorded length of stay in ICU time is defined
from the admission until the time when another patient can take their place. This
could be due to it being recorded primarily for reimbursement purposes.

Lets set all the negative values of `hospital_los_after_icu` to zero:
```{r}
data[data$hospital_los_after_icu < 0, "hospital_los_after_icu"] <- 0
```

Plot the densities for the new variable for dead and censored patients:
```{r fig.height=5}
data %>% ggplot(aes(x = hospital_los_after_icu, fill = censor_flg, alpha = 0.2))+
        geom_density()+
        theme_classic()
```
Here we see that the censored patients had generally longer stays after ICU then the
patients that died. The patients with values of zero and close to zero probably died
in the ICU and the values we calculated are just due to the different rounding /
recording of the hospital / ICU length of stay. 
The variable `icu_exp_flg` tells us if a patient died in the ICU. We could therefore
 set `hospital_los_after_icu` to zero for all patients that died in the ICU.

While this would lead to more truthful values it could also lead to some bias, since we
only manipulate certain values and would reduce the variance for values around zero,
while leaving it the same for values above.

Lets look at the values we would set to zero:
```{r}
data %>% filter(icu_exp_flg == 1) %>% 
        select(hospital_los_after_icu) %>% 
        summary()
```
There is at least one value (the maximum), that is over one and therefore unlikely to
be due to rounding error.

Lets look at all values that over one:
```{r}
data %>% filter(icu_exp_flg == 1 & hospital_los_after_icu > 1) %>% 
        select(hospital_los_after_icu, icu_los_day, hospital_los_day, mort_day_censored) %>% 
        arrange(desc(hospital_los_after_icu)) %>% 
        tibble()
```
For all of the values the time from ICU admission to death is very close to the
length of stay in the ICU, while the length of stay in the hospital is longer.
These could be cases where the patient was already in the hospital for some time 
before being admitted to the ICU. So for our purposes this information does not seem 
relevant and we can set these values to zero. We will leave the values below 1 as they are.
```{r}
data[data$icu_exp_flg == 1 & data$hospital_los_after_icu > 1, "hospital_los_after_icu"] <- 0
```

Lets look at the `age` distribution for all patients:
```{r fig.height=5}
data %>% ggplot(aes(x = age))+
        geom_density()+
        scale_x_continuous(breaks = seq(0, 100, 5))+
        theme_classic()
```
There are three peaks, one in early 20s, one late 40s and one around 80.
One could imagine, that maybe for the younger patients its more likely due to accidents,
due to them taking more risks.

Lets look at the `age` distribution for dead and censored patients:
```{r fig.height=5}
data %>% ggplot(aes(x = age, fill = censor_flg, alpha = 0.2))+
        geom_density()+
        scale_x_continuous(breaks = seq(0, 100, 5))+
        theme_classic()
```
Patients that are older seem to have lower survival probabilities (which doesnt surprise).

Lets look at `service_unit` and `service_num`:  

- `service_unit`: type of service unit (character: FICU, MICU, SICU)  
- `service_num`: service as a numeric (binary: 0 = MICU or FICU, 1 = SICU)  

MICU = Medical ICU

SICU = Surgical ICU

FICU = Finard ICU; this is a term specific to Beth Israel Deaconess Medical Center where MIMIC-II data were collected (https://link.springer.com/chapter/10.1007/978-3-319-43742-2_21)
```{r}
table(data$service_unit, useNA = "always")
```
The source above states, that FICU is a special kind of MICU and therefore they
relabeled it as MICU. I will do the same here:
```{r}
data <- data %>% mutate(service_unit = recode_factor(service_unit, "FICU" = "MICU"))
```

`service_num` contains the same information but in numeric format. We will therefore drop it:
```{r}
data <- data %>% select(!service_num)
```

Lets plot the density of the patient ages split by `service_unit`: 
```{r fig.height=5}
data %>% ggplot(aes(x = age, fill = service_unit, alpha = 0.2))+
        geom_density()+
        scale_x_continuous(breaks = seq(0, 100, 5))+
        theme_classic()
```
We see that more of the young patients where assigned to SICU then MICU, this
supports our hypothesis above, that for the younger people the ICU visit might
more likely be due to accidents, which would more often have to be treated by surgery.  
For the peak around 80 its the same, although the difference is quite small.
One could imagine that at an old age accidents increase a bit, due to reduced physical
abilities e.g. falls etc.

Lets look at the above, but also split by gender (1 = male, 0 = female):
```{r fig.height=5}
data %>% ggplot(aes(x = age, color = paste(service_unit, gender_num)))+
        geom_density()+
        scale_x_continuous(breaks = seq(0, 100, 5))+
        theme_classic()
```
Here we see that for the peak in the 20s, its mostly young men, that are assigned to SICU.
This would make sense, since they generally take more risks / are more involved in
accidents. At an older age this is reduced. At the peak around 45 the men are more assigned to MICU.
For the peak around 80 it seems to be mostly women that are assigned to SICU, could be
due to falls as speculated. In general the older patients seem more likely to be women
and the younger patients more likely to be men.  

Lets also look at a histogram to see the total number of records in each category over time:
```{r fig.height=5}
data %>% ggplot(aes(x = age, fill = paste(service_unit, gender_num)))+
        geom_histogram(position = "dodge")+
        scale_x_continuous(breaks = seq(0, 100, 5))+
        theme_classic()
```
We see the same tendencies as in the density plot. In total number the men that are
assigned to SICU have almost always the highest count until after age 75. This might
be because men have lower life expectancy.

Lets look at the variables describing weekday and time of admission:  
- `day_icu_intime`: day of week of ICU admission (character)  
- `day_icu_intime_num`: day of week of ICU admission (numeric, corresponds with day_icu_intime)  
- `hour_icu_intime`: hour of ICU admission (numeric, hour of admission using 24hr clock)  

We will drop `day_icu_intime_num`, since its the same as `day_icu_intime`, but in numeric.
```{r}
data <- data %>% select(!day_icu_intime_num)
```

Reorder the factor levels for `day_icu_intime`:
```{r}
#look at the levels
data$day_icu_intime %>% levels()
#there are trailing spaces; lets remove them:
data <- data %>% mutate(day_icu_intime = as.factor(str_trim(day_icu_intime)))
#reorder the strings
data <- data %>% mutate(day_icu_intime = fct_relevel(day_icu_intime, levels=c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'))) %>% arrange(day_icu_intime)
```
Plot the number of ICU admissions per day:
```{r fig.height=5}
data %>% ggplot(aes(x = day_icu_intime))+
        geom_bar()+
        theme_classic()
```
We see a decreased number of admissions on Friday and Sunday, and an increased
number on Saturday. On Monday and Thursday the number is slightly increased compared
to Tuesday and Wednesday.

Lets look at number over time of day:
```{r fig.height=5}
data %>% ggplot(aes(x = hour_icu_intime))+
        geom_bar() +
        scale_x_continuous(breaks = seq(0,23,1)) +
        theme_classic()
```
We can see that there is an increase in cases during the night, which makes sense,
as then, the ICU is probably the only available health service.

And now look the same distribution but over the week and split for service unit:
```{r fig.height=5}
data %>% ggplot(aes(x = hour_icu_intime, fill = service_unit, alpha = 0.2))+
        geom_density(kernel = "epanechnikov", bw = 3) +
        facet_grid(~ day_icu_intime)+
        theme_classic()
```
This pattern is more or less the same over the week. On Saturday morning there
seems to be an increase (in SICU and MICU), maybe due to alcohol related accidents 
/ poisoning.  
Generally it look like in the first half of the day SICU receives most of its patients,
while in the second half MICU receives most of its patients.

Lets look at the clinical scores used to predict mortality:  
- `sapsi_first`: first SAPS I score (numeric)  
- `sofa_first`: first SOFA score (numeric)

Lets plot the density of the SAPS I score split by 28 day mortality, to see if
it is able to predict "short term mortality:
```{r fig.height=5}
data %>% ggplot(aes(x = sapsi_first, fill = day_28_flg, alpha = 0.2)) +
        geom_density() +
        theme_classic()
```
The two distributions are show quite some overlap.

Repeat the same for the SOFA score:
```{r fig.height=5}
data %>% ggplot(aes(x = sofa_first, fill = day_28_flg, alpha = 0.2)) +
        geom_density() +
        theme_classic()
```
There is even more overlap.

Is there a correlation between the two scores?
```{r}
cor.test(data$sapsi_first, data$sofa_first)
```
There is a significant correlation. If we will have to choose one of the two SAPS I
might be more suitable (at least to predict short term mortality)

Lets look at `iv_day_1`: input fluids by IV on day 1 (mL, numeric).
I would suspect, that patients with an IAC would have generally higher values.

Lets plot the density, split by `aline_flg`:
```{r fig.height=5}
data %>% ggplot(aes(x = aline_flg, y = iv_day_1)) +
        geom_boxplot() +
        theme_classic()
```
Patients with IAC generally received a bit higher volumes of i.v. liquids. The
distributions for both groups are quite right skewed.


### Correlations

Lets look at pairwise correlations between all the variables:
```{r}
cormatr <- data %>% filter(complete.cases(data) == TRUE) %>% 
        mutate(service_unit = as.numeric(service_unit),
               day_icu_intime = as.numeric(day_icu_intime)) %>% 
        as.matrix() %>% 
        rcorr()

ggcorrplot(cormatr$r, p.mat = cormatr$P, hc.order = TRUE, type = "upper")
```
 We will go through the variables with strong correlations from top to bottom:  
 - `service_unit` has a decently strong (negative, which is meaningless in this context, since
 `service_unit` is an unordered factor) correlation with `resp_flag`.  
 Lets produce a contingency table of the two:
```{r}
table(data$service_unit, data$resp_flg)
```
It looks like over proportionally many of the patients with respiratory problems 
where assigned to MICU. This makes sense, since we could assume that the visit could
have something to do with the respiratory problems, which are usually not treated
surgically. 

There is also a correlation with `aline_flg`. Produce a contingency table:
```{r}
table(data$service_unit, data$aline_flg)
```
In SICU most patients received IAC, while in MICU most did not.

- `aline_flg` has a moderately strong correlation with `abg_count`.
We had not previously looked at this variable more closely:
`abg_count`: arterial blood gas count (number of tests, numeric)
Plot a histogram :
```{r fig.height=5}
data %>% ggplot(aes(x = abg_count, fill = aline_flg, alpha = 0.2))+
        geom_density() +
        theme_classic()
```
It looks like for patients that had no IAC also less blood gas counts where done.
This is probably confounded by the time the patient stayed in the hospital, since
for shorter stays it makes less sense to apply an IAC and there is less time to 
conduct blood gas analyses.  
There are also less strong correlations with the length of stay in the hospital 
or ICU respectively, which seems to point into the same direction.  
There are also less strong  correlations with the SAPS I and SOFA scores.
This makes sense as well, since a worse condition at admission, makes an IAC more
necessary.
The distribution is right skewed.

- `hospital_los_after_icu` correlates with `hospital_los_day`, which makes sense,
since `hospital_los_after_icu` + `icu_los_day` = `hospital_los_day`.
We will drop `hospital_los_day` before we beginn with the modeling.

- `hospital_los_day` correlates with `abg_count` as well and could be explained in a similar fashion.
it is also correlated with `icu_los_day`, which makes sense since icu is a subset of the hospital
so to say (which is why we calculated `hospital_los_after_icu`).

- `censor_flg` and `mort_day_censored` correlate with each other and both correlate
with `icu_exp_flg`, `hosp_exp_flg` and `day_28_flg`. We have already
discussed this above.

- `chloride_first` correlates with `sodium_first` might be that both are influenced by
NaCl intake.

- `abg_count` correlates with `icu_los_day`, again probably due to longer stays 
in the icu leading to more blood cell counts being made.

- `day_28_flg`, `hosp_exp_flg` and `icu_exp_flg` all correlate with each other,
which makes sense since they overlap, meaning somebody that died in the icu also
died in the hospital and possibly within 28 days after icu admission.

## Imputation

As discussed the `bmi` variable will not be imputed and now dropped from the dataset,
since it is correlated quite strongly with `weight_first`. It would also probably not
be of use for imputation of `weight_first`, since in most cases the `bmi` is also NA,
where `weight_first` is NA.
```{r}
data <- data %>% select(!bmi)
```


For the other missing values we will use Multivariate Imputation by Chained Equations (mice).
We will use all the data (except the 2 variables we have already dropped) for the
imputation:
```{r}
imp <- mice(data, print=F, seed = 1)
```

Lets look at the imputed values of `spo2_1st` (which we had previously set to NA,
since they where exceptionally low).
```{r}
imp$imp$spo2_1st
```

All of them are quite high / in the normal range, except one iteration for
one data point at 71.

Lets show some overviews fo the imputed (red) and the other data (blue):
```{r}
stripplot(imp)
```

```{r}
densityplot(imp)
```

The imputed data seems to be quite close to the other data and therefore plausible.

Get the imputed data:
```{r}
data_imp <- complete(imp)
```

## Model 1: death within 28 days as outcome variable

We will fit a logistic regression model with `day_28_flg` as outcome, with the goal to identify
which of the variables are correlated with death within 28 days after icu admission.

### Selection of Variables

First we will exclude variables that contain information about the treatment outcome,
such as length of stay in icu / hospital / death / censoring etc.. Since they are only available "after the fact" /
when the outcome of the treatment is already (more) clear:
```{r}
data_28d <- data_imp %>% select(!c(hospital_los_after_icu, 
                                   hospital_los_day, 
                                   icu_los_day, 
                                   hosp_exp_flg, 
                                   icu_exp_flg, 
                                   censor_flg, 
                                   mort_day_censored))
```

By excluding these variables, we also get rid of most of the variables, that showed
strong correlations with other independent variables.

We will now fit a first model and then investigate the variance inflation factors
to assess multicolinearity and decide if any further variables should be excluded from the model.

For example we we could consider to only include one clinical score (SOFA I or SAPSI)

## Exploratory Modeling

I realized that we probably should not fit a model with `hour_icu_intime` as integer,
since we would not expect a linear relationship with death over time, but there might be
some times where generally more severe cases arrive at the icu.
I will transform it to a factor:
```{r}
data_28d$hour_icu_intime <- as.factor(data_28d$hour_icu_intime)
```

Fit first model:
```{r}
mlreg_1 <- glm(day_28_flg ~ ., family = "binomial", data = data_28d)

summary(mlreg_1)
```
For `hour_icu_intime` there is one of the factor levels, that shows a 
significant correlation with the outcome. I would expect this to be a coincidence,
since we have 24 levels, we would expect at least one of them to have a p-value with the outcome
below 0.05 just by chance.

Lets plot `hour_icu_intime` against the relative frequencies of the outcomes:
```{r fig.height=5}
data_28d %>% ggplot(aes(x = hour_icu_intime, fill = day_28_flg)) +
        geom_bar(position="fill") +
        theme_classic()
```
There doesnt look to be any real tendency here.
We will drop this variable from the model.

Lets repeat the same plot with `day_icu_intime`:
```{r fig.height=5}
data_28d %>% ggplot(aes(x = day_icu_intime, fill = day_28_flg)) +
        geom_bar(position="fill") +
        theme_classic()
```
There dont seem to be any patterns here either.

Exclude the two variables:
```{r}
data_28d <- data_28d %>% select(!c(day_icu_intime,
                                   hour_icu_intime))
```

Fit model:
```{r}
mlreg_2 <- glm(day_28_flg ~ ., family = "binomial", data = data_28d)

summary(mlreg_2)
```

Lets have a look at possible multicolinearity within the remaining data:
```{r}
vif(mlreg_2) %>% sort()
```
In general values under 5 are ok.

We can see that `sodium_first` and `chloride_first` have the highest values,
which might be due to their colinearity, as observed previously. The same could 
be true for `bun_first` and `creatinine_first`, or `tco2_first` and `pco2_first`.

pco2 measures the co2 in the blood and tco2 measures the total co2 in the blood 
(including co2 solved in water).

Since in the above model the coefficient for `sodium_first` doesnt significantly
differ from zero and the one for `chloride_first` does, we will now remove `sodium_first`
from the dataset:
```{r}
data_28d <- data_28d %>% select(!c(sodium_first))
```

Fit model:
```{r}
mlreg_3 <- glm(day_28_flg ~ ., family = "binomial", data = data_28d)

vif(mlreg_3) %>% sort()
```
This reduced the vif value for `chloride_first`.

```{r}
summary(mlreg_3)
```

We will now write a backwards selection function, which in every step removes the variable
with the highest p-value, until all p-values are below 0.05 (excluding the intercept,
since we cannot remove that).

In order for my function to work properly I will reformat the variables `service_unit`
and rename `iv_day_1`:
```{r}
data_28d <- data_28d %>% mutate(service_unit = as.factor(case_when(service_unit == "SICU" ~ 1,
                                             service_unit == "MICU" ~ 0)))

data_28d <- data_28d %>% rename(iv_day_1st = iv_day_1)
```

Write backwards selection function:
```{r}
back_sel_logreg <- function(dataset){
        #set up table to collect statistics
        stats <- data.frame(matrix(nrow = 0, ncol = 5))
        colnames(stats) <- c("Iteration", "removed", "pval", "res_dev", "AIC")
        #set up variables
        pmax <- 1
        iteration <- 1
        while (pmax > 0.05) {
                #fit model / extract coefficients etc
                model <- glm(day_28_flg ~ ., family = "binomial", data = dataset)
                coefs <- coef(summary(model)) %>% data.frame() %>% rename(., pval = "Pr...z..") %>% .[-1,]
                #find highest pvalue, to exclude
                to_exc <- coefs[which(coefs$pval == max(coefs$pval)),]
                #save stats
                stats <- rbind(stats, 
                                   c(iteration, 
                                     rownames(to_exc),
                                     to_exc$pval,
                                     model$deviance,
                                     model$aic))
                #update variables
                pmax <- to_exc$pval
                iteration <- iteration + 1
                
                #get name of variable to remove
                to_exc_name <- rownames(to_exc) %>% str_remove(., "[1-9]$")
                #remove variable from dataframe
                dataset <- dataset %>% select(!to_exc_name)
        }
        return(list(stats, model))
}
```

Run the function:
```{r}
back_sel <- back_sel_logreg(dataset = data_28d)
```
The function returns a list containing the smallest model and a dataframe that documents
the process of removing variables:
```{r}
back_sel[[1]]
```

Lets make some plots with this.
Save and reformat:
```{r}

back_sel_logreg_stats <- back_sel[[1]]
colnames(back_sel_logreg_stats) <- c("iteration", "removed", "pval", "res_dev", "AIC")
back_sel_logreg_stats <- back_sel_logreg_stats %>% 
        mutate(iteration = as.numeric(iteration),
               pval = as.numeric(pval),
               res_dev = as.numeric(res_dev),
               AIC = as.numeric(AIC))
```

Display the p values of the removed variables in the respective models:
```{r}
back_sel_logreg_stats %>% ggplot(aes(x = reorder(removed, iteration), y = pval, group=1)) +
        geom_line() +
        theme_classic() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
        geom_hline(yintercept = 0.05, color = "red")
```
In this plot we can see the order in which the variables where removed and "how
close" they where to give a significant contribution to the respective models.
The cut off of p = 0.05 is marked in red.
The most relevant part here is probably, the few most right data points. We see that
in the next to last model, the coefficient for `renal_flg` was close to differ 
significantly from zero. The next variable that would be removed would be 
`chloride_first`.

Display the AIC and Residual deviances in the respective models:
```{r}
back_sel_logreg_stats[1:17,] %>% ggplot(aes(x = iteration)) +
        geom_line(aes(y = AIC, color = "red")) +
        geom_line(aes(y = res_dev, color = "blue")) +
        theme_classic() +
        ylab("Measure") +
        scale_x_continuous(breaks = 1:20) +
        scale_color_manual(values=c("red", "blue"), 
                       name="Measure",
                       labels=c("AIC", "Residual Deviance"))
```
We can see that over the course of removing the "non significant" variables, 
the Residual Deviance increased monotonically, while the AIC got smaller and then
increased a bit when `renal_flg` was removed.

We would expect the residual deviance to rise and the AIC to get lower, when removing
the non significant variables from the model. That the AIC rises a little bit, when we
remove `renal_flg` could be an indication, that it might be relevant to the outcome
and might also have been found to be significant with more data.

Lets look at the model, that only contains variables that contribute significantly to
the model:
```{r}
summary(back_sel[[2]])
```
We see that the Intercept is not significant. This is not a problem, since it makes
a prediction for patients, that are zero years old, where measured to have a body
temperature of zero F etc. so it doesnt have a practical meaning here.

We will now go trhough the included variables and discuss them:

"General" patient attributes:
- Age
- Gender
- Weight

Higher age is quite a well known risk factor. So is male gender, with males
having generally a lower life expectancy.

What surprised me is the that the coefficient of weight is negative, meaning lower
weight increases the risk for death within 28 days. I would have expected the opposite,
since overweight is quite well known to produce cardiovascular problems etc. It is
possible however that because the model also includes the flags for stroke and arterial 
fibrilation, this is mostly 
adjusted for, and a higher weight means more "reserves" for recovery.

I would suspect that the BMI, we had previously dropped from the data, since there 
where too many missing values would be a better predictor, since it is probably 
more medically relevant, as the height differences between patients are adjusted for.

We will fit two models for these variables with `day_28_flg` as outcome.
We will drop all the records, where bmi is missing. This should not be a problem, 
as it seems like the values are missing randomly:

Load in the data set again:
```{r}
data_w_bmi <- read.csv(file = "Data/full_cohort_data.csv",
                 stringsAsFactors = TRUE)
```

Subset:
```{r}
data_w_bmi <- data_w_bmi %>% 
        select(day_28_flg, bmi, weight_first, gender_num, age) %>% 
        filter(complete.cases(.))
```

Fit the models:

- BMI
```{r}
mlreg_bmi <- glm(day_28_flg ~ bmi, family = "binomial", data = data_w_bmi)
summary(mlreg_bmi)
```

- weight
```{r}
mlreg_w <- glm(day_28_flg ~ weight_first, family = "binomial", data = data_w_bmi)
summary(mlreg_w)
```

Both variables have a significant correlation with the outcome. Weight has a lower p-value
and the AIC for the model with weight is lower then for the model with bmi. This is
unexpected.

What probably confounds the weight the most, is gender. Lets fit two models that also include gender.

- BMI
```{r}
mlreg_bmi2 <- glm(day_28_flg ~ bmi + gender_num, family = "binomial", data = data_w_bmi)
summary(mlreg_bmi2)
```

- weight
```{r}
mlreg_w2 <- glm(day_28_flg ~ weight_first + gender_num, family = "binomial", data = data_w_bmi)
summary(mlreg_w2)
```
These results are quite interesting. In the model with weight, the gender
variable is not significant. I suspect, that this is due to the weight variable
acting also as a proxy for gender here, so since it also contains some information
about gender, the addition of the gender variable doesnt add anything.

Clinical Scores:
- SOFA score
- SAPS I score

These we would also expect to be included in the model, since they where developed to
assess the risk for mortality in the ICU (SOFA) and to assess the general physiological
state of patients (SAPS I).

Diseases / Events / Diagnoses:
- Atrial fibrillation
- Stroke
- Malignancy
- Respiratory Disease

It is not quite clear to me why these are included in the model, while others did not
make it in (such as Congestive heart failure). It is possible that for some the
lab values give the same information, but preciser, 
e.g. for renal disease => Creatinine / Blood Urea Nitrogen give information not only
on the presence of a renal disease, but also its severity.

Lab values:
- Heart Rate
- Temperature
- Hemoglobin
- Chloride
- Blood Urea Nitrogen
- Creatinine

Heart rate and temperature seem obvious to be important for the acute outcome.
For the rest I lack the domain knowledge to discuss much about it.

## Check model assumptions

### Linearity assumption

For the numeric variables used in the model we will have to check if there is 
a linear relationship with the logit of the outcome.

filter for numeric predictors:
```{r}
data_28d_linas <- data_28d %>% select(age, weight_first, sapsi_first, sofa_first, hr_1st, temp_1st, hgb_first, chloride_first, bun_first, creatinine_first)
```

Get predicted probabilities from model:
```{r}
data_28d_linas <- predict(back_sel[[2]], type = "response") %>% bind_cols(pred_p = ., data_28d_linas)
```

Add the logit to the data:
```{r}
data_28d_linas <- data_28d_linas %>%
  mutate(logit = log(pred_p/(1-pred_p))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
```


```{r}
data_28d_linas %>% filter(predictors != "pred_p") %>% ggplot(aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```
A linear fit seems to be the most reasonable for all variables, however for some there is quite a lot
of noise e.g. `hr_1st` and `temp_1st`.

For `creatinine_first` it is hard to see. Therefore, we will repeat the plot without some of the outliers:
```{r}
data_28d_linas %>% filter(predictors == "creatinine_first") %>% ggplot(aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() +
  ylim(c(0, 2.5))      
```
Doesn't look to well.

Lets see what happens if we exclude the variables with the least linear fit:

- `hr_1st`
- `temp_1st`
- `creatinine_first`

```{r}
mlreg_nlexcl <- glm(day_28_flg ~ age + gender_num + weight_first + sapsi_first + 
    sofa_first + afib_flg + stroke_flg + mal_flg + resp_flg + hgb_first + chloride_first + bun_first, 
    family = "binomial", 
    data = data_28d)
summary(mlreg_nlexcl)
```
In this model `chloride_first` and `bun_first` are "no longer significant". Maybe they
only contribute significantly if adjusted with one of the values we have excluded.
My guess would be on `creatinine_first`, just because it is the only one we removed that
is a lab value and is therefore more likely to interplay with these two.

Lets introduce `creatinine_first` again:
```{r}
mlreg_nlexcl2 <- glm(day_28_flg ~ age + gender_num + weight_first + sapsi_first + 
    sofa_first + afib_flg + stroke_flg + mal_flg + resp_flg + hgb_first + chloride_first + bun_first + creatinine_first, 
    family = "binomial", 
    data = data_28d)
summary(mlreg_nlexcl2)
```
This seems to be the case, as in this model their coefficients are significantly different
from zero again (or close to it for `chloride_first`).

Should we drop them or leave them in the model????

### Influential values

```{r}
plot(back_sel[[2]], which = 4, id.n = 3)
```
Looks ok.

### Multicollinearity

We had alreads looked at this before we fit this model. Lets look at it again
with only the included variables:
```{r}
vif(back_sel[[2]]) %>% sort()
```
This is fulfilled as none of the values are over 5.




```{r}
back_sel[[2]] %>% summary()
```


## Model 2: Predict the length of stay in the ICU

We will fit a linear regression model with `icu_los_day` as outcome, with the goal 
to predict the length of stay in ICU after admission. We could imagine a scenario,
where we could use this information in order to better plan our resources, e.g. if
we dont have enough beds according to our prediction, we could start organizing
more beds ahead of time. This would make it irrelevant if a patient dies or
survives.

Lets have a closer look at the time of death and LOS (length of stay) in the ICU:
```{r}
data_imp %>% filter(icu_exp_flg == 1) %>% 
        select(icu_los_day, mort_day_censored) %>% 
        mutate(time_mort_to_los = icu_los_day - mort_day_censored) %>% head()
```
We had already discussed this point above. I assume, the LOS in the ICU was captured such that
it represents, when the bed could be used again for the next patient or when the body was removed from it, since it is always a bit higher then the time of death in patients that died.
This would justify using this variable like this.

Lets plot the density curve of the difference between the time of death and LOS:
```{r}
data_imp %>% filter(icu_exp_flg == 1) %>% 
        select(icu_los_day, mort_day_censored) %>% 
        mutate(time_mort_to_los = icu_los_day - mort_day_censored) %>% 
        ggplot(aes(x = time_mort_to_los))+
        geom_density() +
        theme_classic()
```
In most cases it is between 0 and 1 days. There are some negative values, wich means one of
the two variables was captured wrongly. Since we cannot find out which one is correct,
we will just leave the data as it is.

Create dataframe with relevant data:
```{r}
data_los_icu <- data_imp %>% select(!c(hospital_los_after_icu, 
                                   hospital_los_day, 
                                   mort_day_censored, 
                                   hosp_exp_flg, 
                                   censor_flg,
                                   day_28_flg))
```

# weiter DATA Partition

Since our goal is prediction we will do a threeway split, train:test:validation (6:2:2)
```{r}
createDataPartition(data_los_icu$log_icu_los_day, times = 3, p = c(6,2,2))
```


```{r}
data_los_icu$icu_los_day %>% summary()
```

Lets show the distribution of the outcome variable:
```{r}
data_los_icu %>% ggplot(aes(x = icu_los_day)) +
        geom_density() +
        theme_classic()
```
The outcome is not normally distributed. This is a problem if we want to use linear regression.
Lets try to transform it in order to solve this. Alternatively we could use a Poisson model,
since we can formulate the problem as how many patients are released from ICU within X days.

Show distribution of Log transformed `icu_los_day`:
```{r}
data_los_icu %>% ggplot(aes(x = log(icu_los_day))) +
        geom_density() +
        theme_classic()
```
This looks better, even if not perfect.

Lets replace icu_los_day with its logarithmized version:
```{r}
data_los_icu <- data_los_icu %>% 
        mutate(log_icu_los_day = log(icu_los_day)) %>% 
        select(!icu_los_day)
```

Fit a model including all of the available variables:
```{r}
mlreg_los_icu <- lm(log_icu_los_day ~ ., data = data_los_icu[, names(data_los_icu) != "icu_exp_flg"])
summary(mlreg_los_icu)
```

The model is not that good in predicting the los (adj R2 = 0.4622).

The variables that show a coefficient, which significantly differs from zero are among others
the variables that describe measures taken in the ICU: `aline_flg`, `iv_day_1`, `abg_count`
I will exclude those here, since they are not available from the beginning or shortly after the
ICU admission.

So the task here is made more preciser to: How long will this patient that was admitted
to ICU (a few hours ago) stay in ICU? This means we have all the first lab measurements,
demographics, diagnostics and clinical scores available to us.

Exclude these columns:
```{r}
data_los_icu <- data_los_icu %>% select(!c(aline_flg, iv_day_1, abg_count))
```

We should investigate of what importance the service unit is. Does it
just describe how the patient is treated or are there really two different stations
in the hospital, for patients treated surgically or by internal medicine.

How many patients where in which service unit?
```{r}
data_los_icu$service_unit %>% table()
```
Show a violin plot of the log lenght of stay in ICU for each of the two service units:
```{r fig.width=5}
data_los_icu %>% ggplot(aes(y = log_icu_los_day, x = service_unit)) +
        geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
        theme_classic()
```
The horizontal lines represent the quartiles. We see that patients treated surgically
(SICU) stay generally a bit longer.

I will leave this variable in and consider it to provide information about the treatment
and not that there are actual different ICUs in the hospital.

Fit another model:
```{r}
mlreg_los_icu_2 <- lm(log_icu_los_day ~ ., data = data_los_icu[, names(data_los_icu) != "icu_exp_flg"])
summary(mlreg_los_icu_2)
```
The adj R2 decreased quite a bit compared to the last model (probably mostly due to the 
removal of `abg_count`, as this was somewhat of a proxy for LOS, as awe have seen during the exploratory
data analysis).

The variables that show a coefficient, which significantly differs from zero are among others:
- the service unit (which is in line with the plot we made above) 
- the clinical scores. Their coefficient is positive, meaning a higher score (patient is worse
of leads to predicting a longer stay).
- several diagnostic flags: some with positive coefficients (respiratory disease, stroke, 
congestive heart failure) and some with negative coefficients (Coronary artery disease, 
malignancy). So either the problems of people with malignancy and coronary artery disease can be
resolved quickly or they die quickly.

I think there could be a problem with using a linear model for this task. The 
included variables give some kind of information about the health of the 
patient.The outcome however is probably not linearly related to the the health of the person,
but I would assume in some kind of upside down U-shape, such that a very bad condition 
leads to death and therefore a short stay, not good, but not terrible condition leads to
longer stay and good condition again leads to shorter stay. So one variable can 
so to speak either be used to predict death or health for the surviving patients, but not both.

Lets see if LOS is really shorter for people that died in ICU then for people that didnt.

Summary of log LOS for people that died:
```{r}
data_los_icu %>% filter(icu_exp_flg == 1) %>% select(log_icu_los_day) %>% summary()
```
Summary of log LOS for people that survived:
```{r}
data_los_icu %>% filter(icu_exp_flg == 0) %>% select(log_icu_los_day) %>% summary()
```

Actually the mean is higher for people that died. So maybe that's not really a problem.

Exclude the `icu_exp_flg` variable from the data:
```{r}
data_los_icu <- data_los_icu %>% select(!icu_exp_flg)
```

We will now try to manually slect the variables to include.

Create a plot that shows all the numeric variables plotted against the outcome:
```{r}
data_los_icu %>% select_if(is.numeric) %>%
        pivot_longer(cols =  colnames(.[,-21])) %>% 
        ggplot(aes(y = log_icu_los_day, x = value)) +
        facet_wrap(~ name, scales = 'free_x') +
        geom_point() +
        theme_classic() +
        geom_smooth(method = "lm")
```

There are some variables that are skewed. This is not good, since for a linear
model they should be approximately normally distributed. I will try and see if
logarithmizing them helps:
```{r}
data_los_icu %>% select(bun_first, creatinine_first, spo2_1st, wbc_first, weight_first, pco2_first, log_icu_los_day) %>%
        pivot_longer(cols =  colnames(.[,-7])) %>% 
        ggplot(aes( x = log(value))) +
        facet_wrap(~ name, scales = "free") +
        geom_density() +
        theme_classic()
```
This looks better. For `spo2_1st` we will have to find another solution. We could
transform it into a factor variable (e.g. normal [95-99], low normal [89-95], low [< 89])

Lets look at the scatterplot with the logarithmized variables:
```{r}
data_los_icu %>% select(bun_first, creatinine_first, spo2_1st, wbc_first, weight_first, pco2_first, log_icu_los_day) %>%
        pivot_longer(cols =  colnames(.[,-7])) %>% 
        ggplot(aes(y = log_icu_los_day, x = log(value))) +
        facet_wrap(~ name, scales = 'free_x') +
        geom_point() +
        theme_classic() +
        geom_smooth(method = "lm")
```

Lets transform these variables:
```{r}
data_los_icu_t <- data_los_icu %>% 
        mutate(log_bun_first = log(bun_first), 
               log_creatinine_first = log(creatinine_first),
               log_wbc_first = log(wbc_first), 
               log_weight_first = log(weight_first), 
               log_pco2_first = log(pco2_first)) %>% 
        mutate(fact_spo2_1st = as.factor(case_when(spo2_1st >= 95 ~ "normal",
                                         spo2_1st < 95 &  spo2_1st >= 89 ~ "low normal",
                                         spo2_1st < 89 ~ "low"))) %>% 
        select(!c(bun_first, creatinine_first, spo2_1st, wbc_first, weight_first, pco2_first))
```

Lets see if there is a diff in LOS in ICU between the different groups for SPO2:
```{r}
boxplot(data_los_icu_t$log_icu_los_day ~ data_los_icu_t$fact_spo2_1st)
```
This looks promising.

Lets look closer at `hour_icu_intime`:
```{r}
data_los_icu_t %>% ggplot(aes(x = hour_icu_intime, y = log_icu_los_day)) +
        geom_point() +
        geom_smooth(method = "lm") +
        geom_smooth(method = "loess") +
        theme_classic()
        
```
It is not really plausible, that the hour of admission would have a linear 
relationship with the LOS in ICU. However in the plot above it looks like the patients
that come in from about noon until maybe 20 h would generally stay a bit longer 
(there are less patients that stay for shorter times)
However this could be because less patients arrive during this time. 
Here we dont really see the density very well.

Lets look at a boxplot:
```{r}
boxplot(data_los_icu_t$log_icu_los_day ~ data_los_icu_t$hour_icu_intime)
```

There really doesnt seem to be any pattern here.

Lets break the day up into pieces and have a look again.

It would be possible, that people that arrive during the night are worse off, as they
could not wait until the next day to get o the ICU:

```{r}
data_los_icu_t %>% select(log_icu_los_day, hour_icu_intime) %>% 
        mutate(fact_hour_icu_intime = as.factor(case_when(hour_icu_intime <= 6 ~ "night",
                                         hour_icu_intime > 6 &  hour_icu_intime <= 12 ~ "morning",
                                         hour_icu_intime > 12 &  hour_icu_intime <= 20 ~ "afternoon",
                                         hour_icu_intime > 20  ~ "evening",))) %>% 
        ggplot(aes(x = fact_hour_icu_intime, y = log_icu_los_day)) +
        geom_boxplot() +
        theme_classic()
```

The one that differs from the other groups is the morning group, for which the 
LOS in ICU is generally shorter. If we look at the boxplots for each hour
above we see that for 7-9 the mean of the log LOS is lower. However for 
6, 10 and 11 the means are relatively high.

We can check if our reasoning would be consistent with the clinical scores.
```{r}
boxplot(data_los_icu_t$sofa_first ~ data_los_icu_t$hour_icu_intime)
```

```{r}
boxplot(data_los_icu_t$sapsi_first ~ data_los_icu_t$hour_icu_intime)
```

It looks like the scores are generally lower for the patients that arrive in the morning.
(a bit clearer for the sofa score). However this means we should check if the apparent
relationship might just be due to the differnt clinical scores. In that case the
hour of arrival at the ICU would not add anything to a predictive model.

Lets fit a linear model:
```{r}
data_los_icu_t %>% mutate(fact_hour_icu_intime = 
                                  as.factor(case_when(hour_icu_intime > 12 | hour_icu_intime < 6 ~ "rest of day",
                                  hour_icu_intime >= 6 &  hour_icu_intime <= 12 ~ "morning"))) %>% 
        lm(log_icu_los_day ~ fact_hour_icu_intime + sofa_first + sapsi_first, data = .) %>% 
        summary()
```
When adjusted for the two clinical scores, patients that arrive at ICU show no
significantly different log of LOS in ICU then patients that arrive during the rest of the day.

```{r}
data_los_icu_t %>% mutate(day_in_h = case_when(day_icu_intime == "Monday" ~ 0*24,
                                                  day_icu_intime == "Tuesday" ~ 1*24,
                                                  day_icu_intime == "Wednesday" ~ 2*24,
                                                  day_icu_intime == "Thursday" ~ 3*24,
                                                  day_icu_intime == "Friday" ~ 4*24,
                                                  day_icu_intime == "Saturday" ~ 5*24,
                                                  day_icu_intime == "Sunday" ~ 6*24,)) %>% 
        mutate(hour_of_week = day_in_h + hour_icu_intime) %>%
        ggplot(aes(x = hour_of_week, y = log_icu_los_day)) +
        geom_point() +
        scale_x_continuous(breaks = c(0,24,48,72,96,120,144,168)) +
        theme_classic()
```

There is an odd pattern visible, of bent stripes going from top left to bottom right.

Lets look at it with the outcome not logarithmized:
```{r}
data_los_icu_t %>% mutate(day_in_h = case_when(day_icu_intime == "Monday" ~ 0*24,
                                                  day_icu_intime == "Tuesday" ~ 1*24,
                                                  day_icu_intime == "Wednesday" ~ 2*24,
                                                  day_icu_intime == "Thursday" ~ 3*24,
                                                  day_icu_intime == "Friday" ~ 4*24,
                                                  day_icu_intime == "Saturday" ~ 5*24,
                                                  day_icu_intime == "Sunday" ~ 6*24,)) %>% 
        mutate(hour_of_week = day_in_h + hour_icu_intime) %>%
        ggplot(aes(x = hour_of_week, y = exp(log_icu_los_day))) +
        geom_point() +
        scale_x_continuous(breaks = c(0,20,24,48,72,96,120,144,168)) +
        theme_classic() +
        scale_y_continuous(breaks = 1:10, limits = c(0,10)) +
        geom_vline(xintercept = 20, color = "red") +
        geom_hline(yintercept = 2, color = "blue")
```

I think this could have something to do with there being fixed times, when a
patient can be transferred away from the ICU. So the stripes would represent 
patients that are transferred on the same day at around the same time.

This could serve to explain some of the variation in the outcome.
The model should then only predict the patient to leave at certain times, 
which would be based on the time the patient was admitted.

It looks like they are transferred at around 20 h (or at least that is how it
is recorded, it is also possible, that this is when the bed becomes free again
for other patients), since this is more or less when patients where admitted
 (here on Monday), that stayed more or less exactly two days.
 
However, I dont think this can be implemented within a linear model, but it
could be added as a rounding step after the model was fit.

We will leave it in the dataset to do this, but will not include it in the model.

Lets take a look at the day of arrival:
```{r}
boxplot(data_los_icu_t$log_icu_los_day ~ data_los_icu_t$day_icu_intime)
```
It looks like the log LOS is generally higher for patients that arrive Friday til Sunday.

This could be similar to how we argued for the hour of admission. 
The cases on Friday through Sunday could be more severe. Lets look at boxplots
for the clinical scores:
```{r}
boxplot(data_los_icu_t$sapsi_first ~ data_los_icu_t$day_icu_intime)
```
The mean of the SAPS I score is a bit lower on mondays.

```{r}
boxplot(data_los_icu_t$sofa_first ~ data_los_icu_t$day_icu_intime)
```
The mean of the SOFA score is a bit lower on Saturday and Sundays.

So this would speak against confounding by the clinical scores.
So what is the reason?? Weekend = Alcohol = Accidents etc??

Lets look at boxplots of weekend (incl Friday) vs not weekend:
```{r}
data_los_icu_t %>% mutate(we = as.factor(case_when(
        day_icu_intime %in% c("Monday", "Tuesday", "Wednesday", "Thursday") ~ "not weekend",
        day_icu_intime %in% c("Friday", "Saturday", "Sunday") ~ "weekend"))) %>% 
        ggplot(aes(x = we, y = age)) +
        geom_boxplot() +
        theme_classic()
```


Lets create again a correlation matrix:
```{r}
cormatr <- data %>% filter(complete.cases(data) == TRUE) %>% 
        mutate(service_unit = as.numeric(service_unit),
               day_icu_intime = as.numeric(day_icu_intime)) %>% 
        as.matrix() %>% 
        rcorr()

ggcorrplot(cormatr$r, p.mat = cormatr$P, hc.order = TRUE, type = "upper")
```


If we look again at the correlation matrix we had previously 
```{r fig.height=10}
ggcorrplot(cormatr$r, p.mat = cormatr$P, hc.order = TRUE, type = "upper")
```


```{r}
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(data_los_icu[, names(data_los_icu) != "log_icu_los_day"], 
               data_los_icu$log_icu_los_day, 
               sizes=c(1:33), 
               rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```




We will now use the best subset method to find the best combination of variables to fit our Model. For this we will use the olsrr::ols_step_best_subset function:
```{r}
bestsub <- ols_step_best_subset(mlreg_los_icu_2)
#select and print relevant information from output
cbind(n_variables = bestsub[, "n"], 
      var_included = bestsub[, "predictors"], 
      adj_R2 = round(bestsub[, "adjr"], digits = 3),
      AIC = round(bestsub[, "aic"], digits = 3)) %>% 
        kable()
```

We will select the model that yields the highest adj R2




### Poisson Model

Undo the logarithmizing of `icu_los_day`:
```{r}
data_los_icu_po <- data_los_icu %>% 
        mutate(icu_los_day = exp(log_icu_los_day)) %>% 
        select(!log_icu_los_day)
```


```{r}
pmod <- glm(icu_los_day ~ .,
             data = data_los_icu_po[, names(data_los_icu) != "icu_exp_flg"], 
             family = poisson(link = "log"))
summary(pmod)
```












